# HRI-Dance-Project
Git repository to host code for the final project of Human-Robot Interaction, 2020-2021 Radboud. In this project we research whether vocal or gestural interfaces work better for instructing a robot to do a task, which is a sequence of dance moves. This project uses OpenSesame as an experimental framework and contains an implemention of voice and pose recognition of a fixed set of commands. 

This repository contains the folders and files which were necessary to conduct the experiment. The following will eplxain briefly what the folders and files were used for.
## Demo
A video demonstrating a vocal and gestural trial is available at https://youtu.be/ykhMqEoqnYQ

### Administration:
- Consent Letter
- Information Letter 
- Trial Structure: The final trial setup is now in the main text and considered alternatives are indicated in bold.
- Final Movements 

### Experiment:
- Pool Folder: Folder contains the libraries, images, movies, and models which OpenSesame needed to run the experiment. 
- Responses Folder (Data gathered during the experiment. Will be removed at January 1st 2022)
- Experiment instructions
- Analysis.ipynb (Jupyter Notebook with the explained statistical analyses of all results)
- HRIDanceExperiment.osexp (OpenSesame file of the experiment)
- OpenSesame Instructions (For installing OpenSesame)
- Questionnaire results (Will be removed at January 1st 2022)

### Gesture Recognition:
- Gesture Examples Folder
- Training Data Folder
- Code to recognise gestures
- Trained models for recognising gestures

### Motions:
- Animations Nao zip file with videos of all the dance moves.

### Sketch:
- Code for testing if pose and voice recognition work
- Code for recording training data for the gesture recogniser
