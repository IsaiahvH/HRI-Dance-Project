# HRI-Dance-Project
Git repository to host code for the final project of Human-Robot Interaction, 2020 Radboud. In this project we research whether vocal or gestural interfaces work better for instructing a robot to do a task, which is a sequence of dance moves. This project uses OpenSesame as an experiment interface and implements voice and pose recognition. 

This repository contains the folders and files which were necessary to conduct the experiment. The following will eplxain briefly what the folders and files were used for.
## Folders: 
- TODO: Remove the ...Improvements.md and keep updating this README until all TODOs are done.
- TODO: Remove duplicate osexp files.

### Administration:
- Consent Letter
- Information Letter 
- Trial Structure: The final trial setup is now in the main text and considered alternatives are indicated in bold.
- Final Movements 

### Experiment:
- Pool Folder: Folder contains the libraries, images, movies, and models which OpenSesame needed to run the experiment. 
- Responses Folder
- Experiment instructions
- Analysis.ipynb 
- HRIDanceExperiment.osexp
- OpenSesame Instructions
- Questionnaire results

### Gesture Recognition:
- Gesture Examples Folder
- Training Data Folder
- Code to recognise gestures
- Trained models for recognising gestures

### Motions:
- Animations Nao zip file with videos of all the dance moves.

### Sketch:
- Code for testing if pose and voice recognition work
- Code for recording training data for the gesture recogniser
